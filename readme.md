# ğŸ“š SimpleAgent - RAG System for Document Q&A

A local-first Retrieval-Augmented Generation system that answers questions about your PDF documents, with automatic web search fallback when confidence is low.

![screenshot taken while using the app](screenshots/SimpleAgent_screenshot.png)

## ğŸ¯ Features

- **PDF Ingestion** - Upload PDFs, automatically chunked and embedded
- **Semantic Search** - Find relevant content using vector similarity (ChromaDB)
- **Local LLM** - Answers generated by Mistral 7B via Ollama (runs on your GPU)
- **Smart Fallback** - Automatic web search (Tavily) when PDF content is insufficient
- **Confidence Scoring** - Transparent decision-making on source selection
- **Streamlit UI** - Clean web interface for easy interaction

## ğŸ—ï¸ Architecture
```
PDF â†’ Chunking â†’ Embeddings (GPU) â†’ ChromaDB
                                        â†“
User Query â†’ Semantic Search â†’ Confidence Check â†’ LLM Response
                                    â†“ (if low)
                               Web Search (Tavily)
```

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|------------|
| Embeddings | Sentence-Transformers (all-MiniLM-L6-v2) |
| Vector DB | ChromaDB |
| LLM | Mistral 7B via Ollama |
| Web Search | Tavily API |
| UI | Streamlit |
| GPU Acceleration | PyTorch CUDA |

## ğŸ“¦ Installation
```bash
# Clone repository
git clone https://github.com/yourusername/SimpleAgent.git
cd SimpleAgent

# Create virtual environment
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt

# Install Ollama and pull Mistral
# https://ollama.ai
ollama pull mistral:7b
```

## ğŸš€ Usage

1. Start Ollama:
```bash
ollama serve
```

2. Run the app:
```bash
streamlit run streamlit_ui.py
```

3. Open `http://localhost:8501` in your browser

4. (Optional) Add your Tavily API key for web search fallback

## ğŸ“ Project Structure
```
SimpleAgent/
â”œâ”€â”€ data/                     # PDF storage
â”œâ”€â”€ chroma_db/                # Vector database (auto-generated)
â”œâ”€â”€ venv_SimpleAgent/         # Virtual environment
â”œâ”€â”€ __pycache__/              # Python cache
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ readme.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ ingestion.py              # PDF loading and chunking
â”œâ”€â”€ embeddings.py             # Vector embeddings + ChromaDB
â”œâ”€â”€ query_system.py           # Semantic search + confidence scoring
â”œâ”€â”€ llm_handler.py            # Ollama/Mistral integration
â”œâ”€â”€ web_search_handler.py     # Tavily API fallback
â”œâ”€â”€ rag_pipeline.py           # Main orchestration logic
â”œâ”€â”€ main.py                   # CLI entry point
â””â”€â”€ streamlit_ui.py           # Web UI
```

## ğŸ”’ Privacy

All processing happens locally on your machine. PDFs never leave your computer. Only web search queries are sent externally (when fallback is triggered).

## ğŸ“ License

MIT - see [LICENSE](LICENSE) for details